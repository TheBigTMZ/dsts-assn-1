"""Assignment_1_Part_B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N6Udr1-Hl9iCXVD8P6BX67EAsjOIyW8N

# Predictive Modelling of Eating-Out Problem
## Assignment -1
### Data Sceince Technology and Systems
#### Student Name: Tamzid Ibrahim
#### Student ID: u3265713

## Part B
### Predictive Modelling
"""

# Loading the packages requred for the modelling part
import pandas as pd
#import matplotlib.pyplot as plt
#import seaborn as sns
import numpy as np

"""##### **Loading the Zomato Data Set:**

"""

#Specifying the file path of the dataset
file_path = "data/zomato_df_final_data.csv"
#Loading the dataset
zomato = pd.read_csv(file_path)

"""### Part B - I - Feature Engineering

**1. Performing Data cleaning to deal with missing values**
"""

# Using the columns attribute to print the column names
# Here the list() is used to to avoid showing the 'Index' part
print("The column names are:", list(zomato.columns))

# Printing the summary of the dataset
print(zomato.info())
# Printing the null values in the columns with missing values by using isnull and sum methods
print("\nNull values in each column: \n", zomato.isnull().sum())

# Calculating the missing values in each column and calculating the percentage of missing values per column
# Calculating the missing values in each column of the 'zomato' DataFrame
missing_data = zomato.isna().sum()
# Calculating the percentage of missing data for each column
missing_percentage  = (missing_data / len(zomato)) * 100
# Printing the percentage of missing values for each column
print(missing_percentage)

"""Finding the missing values in each column and finding out what percentage of values are missing in those columns.

It can be observed that rating_number, rating_text and votes have the most amount of missing values, all three of the columns have 31.58% missing values.

As rating_number and rating_text are the target values, it is very cruicial on how these are dealth with.

**Dealing with missing values in Target Variables i.e rating_number and rating_text**

The best possible way to deal with the missing values in the target variables, i.e rating_number and rating_text, is to remove them. Even though they represent a significant portion of the dataset (31.58$). These variables are our targets for regression and classification tasks, any missing values would compromise the models and if these target variables are imputed, it may distort the relationship between features and targets leading to unreliable predictions.  Since these ratings are subjective and dependent on actual user inputs, estimating them is not feasible. Therefore, removing rows with missing values ensures data integrity and the creation of more precise models, even at the cost of reducing dataset size.
"""

# Removing the rows with missing rating_number and rating text
zomato_clean = zomato.dropna(subset=['rating_number', 'rating_text'])
# Printing the null values in the columns with missing values by using isnull and sum methods
print("\nNull values in each column: \n", zomato_clean.isnull().sum())

"""The rows with missing target variables have been removed, it can be observed that the missing values in votes have also been removed from the dataset.

**Dealing with the missing values in the cost column**
"""
"""
# Creating a histogram to visualize the distribution of the 'cost' column in the 'zomato_clean' DataFrame
plt.figure(figsize=(10, 6))
# Number of bins have been set to 30 the fill colour to blue
plt.hist(zomato_clean['cost'], bins=30, color='blue', edgecolor='black')
# Setting the title and its font size
plt.title('Distribution of Cost in Zomato_clean', fontsize=16)
# Setting the labels
plt.xlabel('Cost', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
# Showing the grid
plt.grid(True)
plt.show()
"""

"""The histogram of the cost shows that the cost is right-skewed, indicating that there are extreme values (some restaurants are very expensive). To address the problem, imputation will be done by using median of the cost column."""

# Imputing the missing values in the cost column by using the median
# Using the 'fillna' method to replace NaN values in the 'cost' column with the median cost value.
# The assign method is used to modify the existing dataset
zomato_clean = zomato_clean.assign(cost=zomato_clean['cost'].fillna(zomato_clean['cost'].median()))

"""**Dealing with the missing values in the cost_2 column**"""
"""

# Exactly similar to cost column creating a histogram for the cost_2
plt.figure(figsize=(10, 6))
plt.hist(zomato_clean['cost_2'], bins=30, color='blue', edgecolor='black')
plt.title('Distribution of Cost_2 in Zomato_clean', fontsize=16)
plt.xlabel('Cost_2', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.grid(True)
plt.show()
"""


"""The histogram of the cost_2 column is also right skewed, similar to the cost column, median imputation will be done to remove the na values."""

# Similar to that of the cost column, imputing the cost_2 column missing values with median
zomato_clean = zomato_clean.assign(cost_2=zomato_clean['cost_2'].fillna(zomato_clean['cost_2'].median()))

# Printing the null values in the columns with missing values by using isnull and sum method
print("\nNull values in each column: \n", zomato_clean.isnull().sum())

"""The cost column and the cost_2 columns have been sucessfully imputed by their median values.

But there are other features with missing values, both lat and lng has 115 values and type has 21 missing values.

**Dealing with the missing values in type**

As there are only 21 rows with missing values in the type column, the rows with missing values will be dropped.
"""

# Droppin the rows which have NA in type
zomato_clean = zomato_clean.dropna(subset=['type'])

# Printing the null values in the columns with missing values by using isnull and sum methods
print("\nNull values in each column: \n", zomato_clean.isnull().sum())

"""The missing values have been addressed in the previous steps, for the case of target variables rating_number and rating_text, the rows with NAs were completely removed due their critical role in predictive modelling. For the cost and cost_2 columns, median imputation was applied to manage missing and extreme values, given their right-skewed distributions. Additionally, rows with missing values in the type column were dropped due to the small number of missing entries. The missing values in the lat and lng columns will be addressed after assessing their relevance in the modeling process.

**2. Preparing the data for modelling setup**

**Managing the Numerical columns**

To prepare for the modelling, the categorical columns and numerical columns need to be identified
"""

# Identifying categorical variables in the 'zomato_clean' DataFrame
# This includes columns with data types 'object' (strings) and 'bool' (boolean)
categorical_vars = zomato_clean.select_dtypes(include=['object', 'bool']).columns.tolist()
# Identifying numerical variables in the 'zomato_clean' DataFrame
# This includes columns with data types 'float64' and 'int64' (numeric types).
numerical_vars= zomato_clean.select_dtypes(include=['float64', 'int64']).columns.tolist()
# printing the identified categorical and numerical columns
print("Categorical columns are: ", categorical_vars)
print("Numerical columns are: ", numerical_vars)

"""**Dealing with lat, lng, and cost_2**

To identify important numerical variables, a correlation heatmap will be created.
"""
"""

# Creating a correlation heatmap to visualize relationships between numerical variables in the 'zomato_clean' DataFrame
# Setting the size of the figure
plt.figure(figsize=(10, 6))
corr_matrix = zomato_clean[numerical_vars].corr()
# Plotting the heatmap using Seaborn
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", linewidths=0.5)
# Setting the titles
plt.title('Correlation Heatmap of Numerical Variables in Zomato_clean', fontsize=16)
plt.show()
"""


"""The visualized heatmap shows the relationship between the numerical variables in the data, as expected there is a strong negative correlationship between lat (latitude) and lng(longitude). Additionally rating_number is moderately related to vote indicating that high-rated restaurants tend to receive more votes. The cost and cost_2 columns are perfectly correlated since cost_2 is a transformation of cost.

So, lat and lng columns will be removed from the data, additionally the cost_2 will also be removed because there is autocorrelation between cost and cost_2 which is not good for modelling.
"""

# Removing the 'lat', 'lng', and 'cost_2' columns from the zomato_clean dataset
zomato_clean = zomato_clean.drop(columns=['lat', 'lng', 'cost_2'])

"""**Dealing with the skewness of cost column**"""

# The 'np.log1p()' function is used to compute the natural logarithm of (cost + 1)
# which effectively handles any zero values in the 'cost' column and avoids issues with log(0).
zomato_clean['cost_log'] = np.log1p(zomato_clean['cost'])

# Verify if there are any missing values remaining
zomato_clean.isnull().sum()

"""
# Plotting the transformed 'cost_log' column
plt.figure(figsize=(10, 6))
# Setting the bin size to 30
plt.hist(zomato_clean['cost_log'], bins=30, color='green', edgecolor='black')
# Setting the title
plt.title('Distribution of Log-Transformed Cost in Zomato_clean', fontsize=16)
# Setting the xlabel and ylabel
plt.xlabel('Log(Cost)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.grid(True)
plt.show()
"""


"""Log transormation has been applied to the cost column and after plotting, it can be seen that the distribution for the log_cost is much more normal compared to the skewed distribution of the cost column."""

# Dropping the cost column
zomato_clean = zomato_clean.drop(columns=['cost'])

"""The original cost column will be removed

**Managing the categorical columns**

**Dropping unnecessary categorical columns ('address', 'link', 'phone', 'title', 'color', 'cuisine_color')**
"""

# Droppping the categorical columns 'address', 'link', 'phone', 'title', 'color', 'cuisine_color'
zomato_clean = zomato_clean.drop(columns=['address', 'link', 'phone', 'title', 'color', 'cuisine_color'])

"""The address, link, phone, title, color and cuisine_color columns have been dropped from the data. Because they are irrelevant to the modelling task at hand.

**Addressing the Subzone, type, and cuisine column**

Only taking the suburb name from the subzone column

For the type and cuisine colum the lists are converted and properly formatted
"""

# Extract the location or suburb name from the 'subzone' column
# It has been assumed that the first part of the string before any commas is the suburb name
# The 'apply' function is used to apply a lambda function that splits each string by commas
zomato_clean['subzone'] = zomato_clean['subzone'].apply(lambda x: x.split(',')[0] if isinstance(x, str) else x)
# Changing the 'type' and 'cuisine' column from string representation of lists to actual lists
# The 'eval' function is used here to convert the string representation back into a list,
zomato_clean['type'] = zomato_clean['type'].apply(lambda x: eval(x) if isinstance(x, str) else x)
zomato_clean['cuisine'] = zomato_clean['cuisine'].apply(lambda x: eval(x) if isinstance(x, str) else x)

"""**Encoding the groupon column**"""

# Performing one-hot encoding on the 'groupon' column of the 'zomato_clean' DataFrame
# The 'pd.get_dummies()' function creates new binary columns for each category in the 'groupon' column.
zomato_clean = pd.get_dummies(zomato_clean, columns=['groupon'], drop_first=True)

"""

# Viewing the first 5 rows of the zomato cleaned
zomato_clean.head()
"""

"""**Encoding subzone, type and cuisine column**

The cuisine, type and subzone columns are prepared for machine learning analysis, by using one hot encoding.
"""

# Flattening the 'cuisine' and 'type' column by creating new binary features for each unique category
# The 'explode()' function is used to transform each element of the lists in the 'cuisine' and 'type' column into its own row
# Using one hot encoding for both cuisine and type
zomato_encoded = zomato_clean.explode('cuisine')
zomato_encoded = pd.get_dummies(zomato_encoded, columns=['cuisine'], drop_first=True)
zomato_encoded = zomato_encoded.explode('type')
zomato_encoded = pd.get_dummies(zomato_encoded, columns=['type'], drop_first=True)
# Performing one-hot encoding on the 'subzone' column to create binary features for each unique suburb
zomato_encoded = pd.get_dummies(zomato_encoded, columns=['subzone'], drop_first=True)

"""
# Displaying the first few rows to verify the change
zomato_encoded.head()
"""

"""Numerical columns such as lat, lng, and cost_2 were removed due to high correlations that could introduce multicollinearity issues. A log transformation was applied to the cost column to correct skewness, and the original column was dropped. Categorical columns like address, link, phone, and others were deemed irrelevant and removed. The cuisine and type columns, originally stored as lists, were transformed and one-hot encoded to prepare them for analysis. Additionally, the subzone column was cleaned to extract only the suburb names, and one-hot encoding was applied to categorical features such as groupon, subzone, cuisine, and type to prepare the dataset for machine learning models.

### Part B - II - Regression

### Building Linear Regression Models

In this section, we build and evaluate two linear regression models to predict the restaurant ratings based on the features prepared in the previous steps. We will first train a standard linear regression model, followed by a regression model using stochastic gradient descent (SGD).

**3. Building the linear_regression_model_1**
"""

# Importing necessary functions for model training and evaluation
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
# Dropping the target columns ('rating_number' and 'rating_text') from the dataset to create features (X)
# The 'rating_number' column will be our target variable (y) that we want to predict.
X = zomato_encoded.drop(columns=['rating_number', 'rating_text'])
y = zomato_encoded['rating_number']
# Splitting the dataset into training (80%) and testing (20%) sets
# The 'random_state' parameter ensures reproducibility of the split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Importing the Linear Regression model from scikit-learn
from sklearn.linear_model import LinearRegression
# Initializing the first linear regression model
model_regression_1 = LinearRegression()
# Using the fit method to fit the model
model_regression_1.fit(X_train, y_train)
# By using the the predict method we use the model to predict on the X_test values
y_pred = model_regression_1.predict(X_test)
# Calculating the Mean Squared Error for model 1
mse_model_1 = mean_squared_error(y_test, y_pred)
# Printing the Mean Squared Error for model 1
# A lower MSE is good
print("The mean square error of regression_model_1 is", mse_model_1)

"""**4. Building the linear_regression_model_2**"""

# Importing the SGDRegressor from scikit-learn
from sklearn.linear_model import SGDRegressor
# Initialize and train the second linear regression model using Gradient Descent
# 'max_iter=1000': This sets the maximum number of iterations for the algorithm to converge.
# A value of 1000 is chosen to ensure that the model has enough iterations to learn from the data.
# 'learning_rate='constant'': This indicates that the learning rate remains constant throughout the training process,
# which can provide stable convergence, especially in cases where the optimal learning rate is known in advance.
# 'eta0=0.000051': This is the initial learning rate used by the model.
model_regression_2 = SGDRegressor(max_iter=1000, learning_rate='constant', eta0=0.000051)
model_regression_2.fit(X_train, y_train)
# Predicting on the test set with model_regression_2
y_pred_sgd = model_regression_2.predict(X_test)
# Calculating the Mean Squared Error for model 2
mse_model_2 = mean_squared_error(y_test, y_pred_sgd)
print("The mean square error of regression_model_2 is", mse_model_2)

# Standardizing the data to improve the performance of the SGD Regressor or the second model
# The first SGD model had a very high Mean Squared Error (MSE), indicating poor prediction accuracy.
# Standardization scales the features to have a mean of 0 and a variance of 1
# This will help the model converge more effectively and improve its performance
from sklearn.preprocessing import StandardScaler
# Standardizing the data by using the StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Initialize and train the SGD Regressor model again with scaled data
model_regression_2 = SGDRegressor(max_iter=1000, learning_rate='constant', eta0=0.000051)
model_regression_2.fit(X_train_scaled, y_train)
# Predict on the test set by using the standardized model
y_pred_sgd = model_regression_2.predict(X_test_scaled)
# Calculate the Mean Squared Error for model 2 with scaled data
mse_model_2_scaled = mean_squared_error(y_test, y_pred_sgd)
print("The mean square error of regression_model_2_scaled is", mse_model_2_scaled)

"""**5. Reporting the mean square error(MSE) on the test data for both models**

In evaluating the performance of regression models, the Mean Squared Error (MSE) provides a quantitative measure of prediction accuracy.

Comparing the models with their respective MSE

Regression Model 1 (linear regression)

MSE: 0.1323876442737592

An MSE of 0.132 indicates that, on average, the square of the error or the difference between predicted values and actual values is 0.132. Even though this is a relatively low error, a lower error would have been better. It can be concluded that the model has resonably accurate predictions.

Regression Model 2 (SGD Regressor)

MSE: 1.4281444829959446e+22

The extremely high MSE indicates a significant failure in prediction accuracy, likely due to improper convergence of the model.

Regression Model 2 with scaled data (SGD Regressor with Scaled Data)

MSE: 0.13287898284637378

After standardizing the data, the MSE of the SGD regressor improved significantly, making it comparable to regression model 1.

### Part C - III - Classification

**6. Simplifying the problem into binary classification**

To simplify the problem into a binary classification, a binary target variable based on the rating text column has been created, the Class 1 is created to include ratings which are classfied as "Poor" and "Average" and class 2 encompasses ratings which are classified as "Good", "Very Good" and "Excellent". The implementation involves using the apply function along with a lambda expression to convert the categorical ratings into a binary format.
"""

# Creating a binary target variable based on the 'rating_text' column
# We define 'Poor' and 'Average' ratings as class 1, and all other ratings as class 2.
# The 'apply' function with a lambda expression is used to convert the categorical ratings into binary format.
binary_target = zomato_encoded['rating_text'].apply(lambda x: 1 if x in ['Poor', 'Average'] else 2)
# Split the data into train (80%) and test (20%) sets with random_state set to 0 for reproducibility
X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, binary_target, test_size=0.2, random_state=0)

"""**7. Building the logistic regression model or model_classification_3**"""

# Importing the Logistic Regression model from scikit-learn
from sklearn.linear_model import LogisticRegression
# Setting up the classification model
# Max iter sets the maximum number of iterations
# random_state = 0 is used for reproducibility
model_classification_3 = LogisticRegression(max_iter=1000, random_state=0)
# fitting the model on the X_train and y_train
model_classification_3.fit(X_train_class, y_train_class)
# using the predict method to predict
y_pred_class = model_classification_3.predict(X_test_class)
# Checking the model accuracy by using the score method
accuracy_model_classification_3= model_classification_3.score(X_test_class, y_test_class)
# Printing the accuracy of the model
print(f'Accuracy of Logistic Classifier: {accuracy_model_classification_3}')

"""A logistic regression model has been built, to classify the simplified data, after running the model it was observed that the accuracy of the model was 0.844, i.e the model predicted 84% of the data correctly.

**8. Use the confusion matrix to report the results of using the classification model on the test data**
"""

# Importing necessary for evaluation of the model
from sklearn.metrics import confusion_matrix, classification_report
# Computing the confusion matrix
# The confusion matrix summarizes the performance of the classification model
conf_matrix = confusion_matrix(y_test_class, y_pred_class)
# Displaying the confusion matrix
print(f'Confusion Matrix:\n{conf_matrix}')
# creating the classification report for more detailed performance metrics
class_report = classification_report(y_test_class, y_pred_class)
# Printing the classification report
print(f'Classification Report:\n{class_report}')

"""**9. Draw your conclusions and observations about the performance of the model relevant to the classesâ€™ distributions.**

Conclusions and Observations

Class Performance:

Class 1 (Poor and Average): The model performs well, with a high recall of 0.94 and an F1-score of 0.88, indicating that it effectively identifies most instances of this class.

Class 2 (Good, Very Good, Excellent): The model struggles more, showing a lower recall of 0.69 and an F1-score of 0.76. This suggests that many actual Class 2 instances are misclassified as Class 1.

Precision: The precision for both classes is relatively high (0.84 for Class 1 and 0.86 for Class 2), meaning when the model predicts a class, it is mostly correct.

Overall Accuracy: The model achieves an accuracy of 0.84, indicating that it correctly predicts 84% of the test instances.

Summary

While the model is effective at predicting Class 1 ratings, it has room for improvement in identifying Class 2 ratings. The imbalance in class distribution likely contributes to this discrepancy.

**10. Repeating the classification tasks using three models of my choice.**

I have chosen KNN, DecisionTree and Random forest
"""

# Importing the Kneigbours classifier
from sklearn.neighbors import KNeighborsClassifier
# Initializing and training the KNN model
# The number of neighbors (n_neighbors) is set to 5. This parameter can be adjusted based on model performance.
model_knn = KNeighborsClassifier(n_neighbors=5)
# Fitting the model
model_knn.fit(X_train_class, y_train_class)
# Predicting on the test set
y_pred_knn = model_knn.predict(X_test_class)
# Checking model accuracy
accuracy_knn = model_knn.score(X_test_class, y_test_class)
# Creating the classification report
class_report = classification_report(y_test_class, y_pred_class)
# Printing the accuracy
print(f'Accuracy of KNN: {accuracy_knn}')
print(f'Classification Report of KNN:\n{class_report}')


# Importing the Decision Tree classifier
from sklearn.tree import DecisionTreeClassifier
# Initializing and training the decision tree model
model_dt = DecisionTreeClassifier(random_state=0)
# Fitting the model
model_dt.fit(X_train_class, y_train_class)
# Predicting on the test set
y_pred_dt = model_dt.predict(X_test_class)
# Checking model accuracy
accuracy_dt = model_dt.score(X_test_class, y_test_class)
# Creating the classification report
class_report = classification_report(y_test_class, y_pred_class)
# Printing the accuracy
print(f'Accuracy of Decision Tree: {accuracy_dt}')
# Printing the accuracy
print(f'Accuracy of Decision Tree: {accuracy_dt}')

# Importing the random forest classifier
from sklearn.ensemble import RandomForestClassifier
# Initializing and training the Random Forest model
model_rf = RandomForestClassifier(n_estimators=100, random_state=0)
model_rf.fit(X_train_class, y_train_class)
# Predicting on the test set
y_pred_rf = model_rf.predict(X_test_class)
# Checking model accuracy
accuracy_rf = model_rf.score(X_test_class, y_test_class)
# Creating the classification report
class_report = classification_report(y_test_class, y_pred_class)
# Printing the accuracy of the model
print(f'Accuracy of Random Forest: {accuracy_rf}')
# Printing the accuracy of the model
print(f'Accuracy of Random Forest: {accuracy_rf}')

